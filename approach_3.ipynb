{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582102e1",
   "metadata": {},
   "source": [
    "<h1>Importing Libraries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6798d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import interp1d, CubicSpline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba301c",
   "metadata": {},
   "source": [
    "<h1>Loading Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad487ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (178340, 97)\n",
      "Test shape: (12065, 96)\n",
      "Available columns in train: ['timestamp', 'underlying', 'expiry', 'call_iv_23500', 'call_iv_23600', 'call_iv_23700', 'call_iv_23800', 'call_iv_23900', 'call_iv_24000', 'call_iv_24100', 'call_iv_24200', 'call_iv_24300', 'call_iv_24400', 'call_iv_24500', 'call_iv_24600', 'call_iv_24700', 'call_iv_24800', 'call_iv_24900', 'call_iv_25000', 'call_iv_25100', 'call_iv_25200', 'call_iv_25300', 'call_iv_25400', 'call_iv_25500', 'call_iv_25600', 'call_iv_25700', 'call_iv_25800', 'call_iv_25900', 'call_iv_26000', 'put_iv_22500', 'put_iv_22600', 'put_iv_22700', 'put_iv_22800', 'put_iv_22900', 'put_iv_23000', 'put_iv_23100', 'put_iv_23200', 'put_iv_23300', 'put_iv_23400', 'put_iv_23500', 'put_iv_23600', 'put_iv_23700', 'put_iv_23800', 'put_iv_23900', 'put_iv_24000', 'put_iv_24100', 'put_iv_24200', 'put_iv_24300', 'put_iv_24400', 'put_iv_24500', 'put_iv_24600', 'put_iv_24700', 'put_iv_24800', 'put_iv_24900', 'put_iv_25000', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "# Load data\n",
    "train = pd.read_parquet('../train.parquet')\n",
    "test = pd.read_parquet('../test.parquet')\n",
    "sample_sub = pd.read_csv('../sample_submission.csv')\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Available columns in train: {train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cec10e",
   "metadata": {},
   "source": [
    "<h1>Precomputations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b931cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52 IV columns to predict\n",
      "Found 43 feature columns available in both datasets: ['underlying', 'X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']...\n",
      "Note: 'expiry' column found in train but not in test - will use for validation only\n",
      "Found 36 unique strikes\n",
      "Strike price range: 23000.0 - 26500.0\n"
     ]
    }
   ],
   "source": [
    "# Get all IV columns from TEST data\n",
    "iv_columns = [col for col in test.columns if col.startswith(('call_iv_', 'put_iv_'))]\n",
    "print(f\"Found {len(iv_columns)} IV columns to predict\")\n",
    "\n",
    "# Get feature columns that exist in BOTH train and test\n",
    "feature_columns = []\n",
    "if 'underlying' in train.columns and 'underlying' in test.columns:\n",
    "    feature_columns.append('underlying')\n",
    "\n",
    "# Add X{0...41} features that exist in both datasets\n",
    "x_features = [col for col in train.columns if col.startswith('X') and col[1:].isdigit() and col in test.columns]\n",
    "feature_columns.extend(x_features)\n",
    "\n",
    "# Note: expiry is only in train, not in test, so we can't use it as a feature for prediction\n",
    "print(f\"Found {len(feature_columns)} feature columns available in both datasets: {feature_columns[:10]}...\")\n",
    "if 'expiry' in train.columns and 'expiry' not in test.columns:\n",
    "    print(\"Note: 'expiry' column found in train but not in test - will use for validation only\")\n",
    "\n",
    "# Create strike dictionary from TEST columns\n",
    "strike_dict = {}\n",
    "for col in iv_columns:\n",
    "    strike = col.split('_')[-1]\n",
    "    if strike not in strike_dict:\n",
    "        strike_dict[strike] = {'call': None, 'put': None}\n",
    "    \n",
    "    if col.startswith('call_iv_'):\n",
    "        strike_dict[strike]['call'] = col\n",
    "    else:\n",
    "        strike_dict[strike]['put'] = col\n",
    "\n",
    "print(f\"Found {len(strike_dict)} unique strikes\")\n",
    "\n",
    "# Extract numerical strike prices\n",
    "def extract_strike_price(strike_str):\n",
    "    try:\n",
    "        return float(strike_str)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Create strike price mapping\n",
    "strike_prices = {}\n",
    "for strike_str in strike_dict.keys():\n",
    "    strike_prices[strike_str] = extract_strike_price(strike_str)\n",
    "\n",
    "# Sort strikes by price\n",
    "sorted_strikes = sorted(strike_dict.keys(), key=lambda x: strike_prices[x])\n",
    "print(f\"Strike price range: {min(strike_prices.values()):.1f} - {max(strike_prices.values()):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133c1f9",
   "metadata": {},
   "source": [
    "<h1>Prediction function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2c6b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_iv_prediction(data, feature_cols=None, is_train=False):\n",
    "    data = data.copy()\n",
    "    print(f\"\\nProcessing {len(data)} rows...\")\n",
    "    \n",
    "    # Ensure we only use features that exist in the current dataset\n",
    "    if feature_cols:\n",
    "        available_features = [col for col in feature_cols if col in data.columns]\n",
    "        if len(available_features) != len(feature_cols):\n",
    "            print(f\"Note: Using {len(available_features)}/{len(feature_cols)} features available in this dataset\")\n",
    "        feature_cols = available_features\n",
    "    \n",
    "    # Phase 1: Enhanced put-call parity with confidence weighting\n",
    "    print(\"Phase 1: Put-call parity enforcement...\")\n",
    "    parity_fixes = 0\n",
    "    \n",
    "    for strike, cols in tqdm(strike_dict.items(), desc=\"Processing strikes\"):\n",
    "        call_col = cols['call']\n",
    "        put_col = cols['put']\n",
    "        \n",
    "        if call_col in data.columns and put_col in data.columns:\n",
    "            # Calculate confidence based on available data\n",
    "            for idx in data.index:\n",
    "                call_val = data.at[idx, call_col]\n",
    "                put_val = data.at[idx, put_col]\n",
    "                \n",
    "                if pd.isna(call_val) and not pd.isna(put_val):\n",
    "                    data.at[idx, call_col] = put_val\n",
    "                    parity_fixes += 1\n",
    "                elif pd.isna(put_val) and not pd.isna(call_val):\n",
    "                    data.at[idx, put_col] = call_val\n",
    "                    parity_fixes += 1\n",
    "                elif not pd.isna(call_val) and not pd.isna(put_val):\n",
    "                    # Blend values for consistency\n",
    "                    avg_val = (call_val + put_val) / 2\n",
    "                    data.at[idx, call_col] = 0.8 * call_val + 0.2 * avg_val\n",
    "                    data.at[idx, put_col] = 0.8 * put_val + 0.2 * avg_val\n",
    "    \n",
    "    print(f\"Applied {parity_fixes} put-call parity fixes\")\n",
    "    \n",
    "    # Phase 2: Advanced interpolation across strikes\n",
    "    print(\"Phase 2: Cross-strike interpolation...\")\n",
    "    interpolation_fixes = 0\n",
    "    \n",
    "    for idx in tqdm(data.index, desc=\"Interpolating rows\"):\n",
    "        # Collect available IVs with their strikes\n",
    "        available_data = []\n",
    "        for col in iv_columns:\n",
    "            if col in data.columns and not pd.isna(data.at[idx, col]):\n",
    "                strike_price = strike_prices[col.split('_')[-1]]\n",
    "                available_data.append((strike_price, data.at[idx, col]))\n",
    "        \n",
    "        if len(available_data) >= 3:\n",
    "            # Sort by strike price\n",
    "            available_data.sort(key=lambda x: x[0])\n",
    "            strikes, ivs = zip(*available_data)\n",
    "            \n",
    "            # Use cubic spline for smoother interpolation\n",
    "            try:\n",
    "                cs = CubicSpline(strikes, ivs, bc_type='natural')\n",
    "                \n",
    "                # Fill missing values\n",
    "                for col in iv_columns:\n",
    "                    if col in data.columns and pd.isna(data.at[idx, col]):\n",
    "                        target_strike = strike_prices[col.split('_')[-1]]\n",
    "                        interpolated_iv = cs(target_strike)\n",
    "                        \n",
    "                        # Apply bounds checking\n",
    "                        if 0.001 < interpolated_iv < 2.0:\n",
    "                            data.at[idx, col] = interpolated_iv\n",
    "                            interpolation_fixes += 1\n",
    "                        else:\n",
    "                            # Fallback to local averaging\n",
    "                            nearby_ivs = [iv for strike, iv in available_data \n",
    "                                        if abs(strike - target_strike) <= 100]\n",
    "                            if nearby_ivs:\n",
    "                                data.at[idx, col] = np.mean(nearby_ivs)\n",
    "                                interpolation_fixes += 1\n",
    "                            else:\n",
    "                                data.at[idx, col] = np.mean(ivs)\n",
    "                                interpolation_fixes += 1\n",
    "            except:\n",
    "                # Fallback to linear interpolation\n",
    "                f = interp1d(strikes, ivs, kind='linear', bounds_error=False, fill_value='extrapolate')\n",
    "                for col in iv_columns:\n",
    "                    if col in data.columns and pd.isna(data.at[idx, col]):\n",
    "                        target_strike = strike_prices[col.split('_')[-1]]\n",
    "                        interpolated_iv = f(target_strike)\n",
    "                        if 0.001 < interpolated_iv < 2.0:\n",
    "                            data.at[idx, col] = interpolated_iv\n",
    "                            interpolation_fixes += 1\n",
    "                        else:\n",
    "                            data.at[idx, col] = np.mean(ivs)\n",
    "                            interpolation_fixes += 1\n",
    "        \n",
    "        elif len(available_data) >= 1:\n",
    "            # Use nearest neighbor approach\n",
    "            strikes, ivs = zip(*available_data)\n",
    "            row_mean = np.mean(ivs)\n",
    "            \n",
    "            for col in iv_columns:\n",
    "                if col in data.columns and pd.isna(data.at[idx, col]):\n",
    "                    data.at[idx, col] = row_mean\n",
    "                    interpolation_fixes += 1\n",
    "    \n",
    "    print(f\"Applied {interpolation_fixes} interpolation fixes\")\n",
    "    \n",
    "    # Phase 3: ML-based refinement using market features\n",
    "    if is_train and feature_cols and len(feature_cols) > 0:\n",
    "        print(\"Phase 3: ML-based refinement...\")\n",
    "        \n",
    "        # Prepare feature matrix\n",
    "        feature_data = data[feature_cols].copy()\n",
    "        \n",
    "        # Handle categorical variables (expiry only exists in train)\n",
    "        # No special handling needed since expiry is excluded from test features\n",
    "        \n",
    "        # Fill NaN values in features\n",
    "        for col in feature_data.columns:\n",
    "            if feature_data[col].dtype in ['float64', 'int64']:\n",
    "                feature_data[col] = feature_data[col].fillna(feature_data[col].median())\n",
    "            else:\n",
    "                feature_data[col] = feature_data[col].fillna(0)\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        feature_data_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(feature_data),\n",
    "            columns=feature_data.columns,\n",
    "            index=feature_data.index\n",
    "        )\n",
    "        \n",
    "        # Train individual models for each IV column\n",
    "        ml_models = {}\n",
    "        scalers = {}\n",
    "        \n",
    "        for col in tqdm(iv_columns, desc=\"Training ML models\"):\n",
    "            if col in data.columns:\n",
    "                # Prepare training data\n",
    "                valid_rows = ~data[col].isna()\n",
    "                if valid_rows.sum() > 50:  # Need minimum samples\n",
    "                    X = feature_data_scaled.loc[valid_rows]\n",
    "                    y = data.loc[valid_rows, col]\n",
    "                    \n",
    "                    # Train Ridge regression (more stable than RF for this case)\n",
    "                    model = Ridge(alpha=0.1)\n",
    "                    model.fit(X, y)\n",
    "                    ml_models[col] = model\n",
    "                    scalers[col] = scaler\n",
    "        \n",
    "        print(f\"Trained {len(ml_models)} ML models\")\n",
    "        \n",
    "        # Apply ML corrections\n",
    "        ml_corrections = 0\n",
    "        for col, model in tqdm(ml_models.items(), desc=\"Applying ML corrections\"):\n",
    "            if col in data.columns:\n",
    "                X = feature_data_scaled\n",
    "                predictions = model.predict(X)\n",
    "                \n",
    "                # Apply corrections with confidence weighting\n",
    "                # Use enumerate to get proper array indices\n",
    "                for array_idx, idx in enumerate(data.index):\n",
    "                    current_val = data.at[idx, col]\n",
    "                    predicted_val = predictions[array_idx]  # Use array_idx instead of idx\n",
    "                    \n",
    "                    if 0.001 < predicted_val < 2.0:\n",
    "                        # Weighted combination - more conservative\n",
    "                        data.at[idx, col] = 0.9 * current_val + 0.1 * predicted_val\n",
    "                        ml_corrections += 1\n",
    "        \n",
    "        print(f\"Applied {ml_corrections} ML corrections\")\n",
    "    else:\n",
    "        if not feature_cols or len(feature_cols) == 0:\n",
    "            print(\"Phase 3: Skipped (no features available for ML)\")\n",
    "        else:\n",
    "            print(\"Phase 3: Skipped (not training mode)\")\n",
    "    \n",
    "    # Phase 4: Volatility smile consistency enforcement\n",
    "    print(\"Phase 4: Volatility smile smoothing...\")\n",
    "    smoothing_corrections = 0\n",
    "    \n",
    "    for idx in tqdm(data.index, desc=\"Smoothing volatility smiles\"):\n",
    "        # Collect all IVs for this timestamp\n",
    "        all_data = []\n",
    "        \n",
    "        for col in iv_columns:\n",
    "            if col in data.columns and not pd.isna(data.at[idx, col]):\n",
    "                strike_price = strike_prices[col.split('_')[-1]]\n",
    "                all_data.append((strike_price, col, data.at[idx, col]))\n",
    "        \n",
    "        if len(all_data) >= 5:  # Need enough points for smoothing\n",
    "            # Sort by strike price\n",
    "            all_data.sort(key=lambda x: x[0])\n",
    "            \n",
    "            # Apply moving average smoothing\n",
    "            window_size = min(3, len(all_data))\n",
    "            \n",
    "            for i, (strike, col, original_iv) in enumerate(all_data):\n",
    "                start_idx = max(0, i - window_size // 2)\n",
    "                end_idx = min(len(all_data), i + window_size // 2 + 1)\n",
    "                \n",
    "                nearby_ivs = [item[2] for item in all_data[start_idx:end_idx]]\n",
    "                smoothed_iv = np.mean(nearby_ivs)\n",
    "                \n",
    "                # Light smoothing - very conservative\n",
    "                new_iv = 0.95 * original_iv + 0.05 * smoothed_iv\n",
    "                data.at[idx, col] = new_iv\n",
    "                smoothing_corrections += 1\n",
    "    \n",
    "    print(f\"Applied {smoothing_corrections} smoothing corrections\")\n",
    "    \n",
    "    # Phase 5: Final put-call parity enforcement\n",
    "    print(\"Phase 5: Final parity enforcement...\")\n",
    "    final_fixes = 0\n",
    "    \n",
    "    for strike, cols in tqdm(strike_dict.items(), desc=\"Final parity check\"):\n",
    "        call_col = cols['call']\n",
    "        put_col = cols['put']\n",
    "        \n",
    "        if call_col in data.columns and put_col in data.columns:\n",
    "            for idx in data.index:\n",
    "                call_val = data.at[idx, call_col]\n",
    "                put_val = data.at[idx, put_col]\n",
    "                \n",
    "                if not pd.isna(call_val) and not pd.isna(put_val):\n",
    "                    # Final parity adjustment - very light\n",
    "                    avg_val = (call_val + put_val) / 2\n",
    "                    data.at[idx, call_col] = 0.99 * call_val + 0.01 * avg_val\n",
    "                    data.at[idx, put_col] = 0.99 * put_val + 0.01 * avg_val\n",
    "                    final_fixes += 1\n",
    "    \n",
    "    print(f\"Applied {final_fixes} final parity adjustments\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d162f5d",
   "metadata": {},
   "source": [
    "<h1>Validation Score</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21eabdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating validation split...\n",
      "Training set: 142672 rows\n",
      "Validation set: 35668 rows\n",
      "\n",
      "==================================================\n",
      "PROCESSING VALIDATION SET\n",
      "==================================================\n",
      "\n",
      "Processing 35668 rows...\n",
      "Phase 1: Put-call parity enforcement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing strikes: 100%|██████████| 36/36 [00:30<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 0 put-call parity fixes\n",
      "Phase 2: Cross-strike interpolation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating rows: 100%|██████████| 35668/35668 [00:24<00:00, 1431.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 0 interpolation fixes\n",
      "Phase 3: ML-based refinement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ML models: 100%|██████████| 52/52 [00:00<00:00, 56.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained 42 ML models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying ML corrections: 100%|██████████| 42/42 [00:54<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 1329979 ML corrections\n",
      "Phase 4: Volatility smile smoothing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Smoothing volatility smiles: 100%|██████████| 35668/35668 [01:29<00:00, 399.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 1498056 smoothing corrections\n",
      "Phase 5: Final parity enforcement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final parity check: 100%|██████████| 36/36 [00:31<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 392348 final parity adjustments\n",
      "\n",
      "Calculating validation MSE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating MSE per column: 100%|██████████| 52/52 [00:00<00:00, 3985.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Total predictions made: 0\n",
      "Validation MSE (masked points only): 0.000000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create validation split - use simple random split since expiry might not be in test\n",
    "print(\"\\nCreating validation split...\")\n",
    "train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} rows\")\n",
    "print(f\"Validation set: {len(val_df)} rows\")\n",
    "\n",
    "# Apply to validation set\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING VALIDATION SET\")\n",
    "print(\"=\"*50)\n",
    "val_pred = advanced_iv_prediction(val_df, feature_columns, is_train=True)\n",
    "\n",
    "# Calculate MSE only on originally masked validation points\n",
    "print(\"\\nCalculating validation MSE...\")\n",
    "mse_vals = []\n",
    "total_predictions = 0\n",
    "\n",
    "for col in tqdm(iv_columns, desc=\"Calculating MSE per column\"):\n",
    "    if col in val_df.columns and col in val_pred.columns:\n",
    "        # Focus only on points that were originally missing\n",
    "        mask = val_df[col].isna() & val_pred[col].notna()\n",
    "        if mask.any():\n",
    "            se = (val_df.loc[mask, col] - val_pred.loc[mask, col]) ** 2\n",
    "            mse_vals.append(se.mean())\n",
    "            total_predictions += mask.sum()\n",
    "\n",
    "validation_mse = np.mean(mse_vals) if mse_vals else 0\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"Total predictions made: {total_predictions}\")\n",
    "print(f\"Validation MSE (masked points only): {validation_mse:.12f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c4106",
   "metadata": {},
   "source": [
    "<h1>Test data predictions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbac22ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PROCESSING TEST SET\n",
      "==================================================\n",
      "\n",
      "Processing 12065 rows...\n",
      "Phase 1: Put-call parity enforcement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing strikes: 100%|██████████| 36/36 [00:09<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 92735 put-call parity fixes\n",
      "Phase 2: Cross-strike interpolation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating rows: 100%|██████████| 12065/12065 [00:55<00:00, 218.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 283769 interpolation fixes\n",
      "Phase 3: Skipped (not training mode)\n",
      "Phase 4: Volatility smile smoothing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Smoothing volatility smiles: 100%|██████████| 12065/12065 [00:35<00:00, 337.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 627380 smoothing corrections\n",
      "Phase 5: Final parity enforcement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final parity check: 100%|██████████| 36/36 [00:14<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 193040 final parity adjustments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply to test set\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PROCESSING TEST SET\")\n",
    "print(\"=\"*50)\n",
    "test_pred = advanced_iv_prediction(test, feature_columns, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729018e",
   "metadata": {},
   "source": [
    "<h1>Submission</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dec6be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing submission...\n",
      "Final quality checks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quality checks: 100%|██████████| 52/52 [00:00<00:00, 650.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare submission\n",
    "print(\"\\nPreparing submission...\")\n",
    "submission = test_pred[['timestamp'] + iv_columns].copy()\n",
    "submission.columns = sample_sub.columns\n",
    "\n",
    "# Final data quality checks and corrections\n",
    "print(\"Final quality checks...\")\n",
    "for col in tqdm(iv_columns, desc=\"Quality checks\"):\n",
    "    if col in submission.columns:\n",
    "        # Ensure no extreme values\n",
    "        submission[col] = submission[col].clip(0.001, 1.5)\n",
    "        \n",
    "        # Fill any remaining NaN values\n",
    "        if submission[col].isna().any():\n",
    "            print(f\"Warning: {submission[col].isna().sum()} NaN values found in {col}, filling with median\")\n",
    "            submission[col] = submission[col].fillna(submission[col].median())\n",
    "\n",
    "# Verify no missing values\n",
    "missing_values = submission.isna().sum().sum()\n",
    "assert missing_values == 0, f\"Missing values detected: {missing_values}\"\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2dd750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL RESULTS\n",
      "==================================================\n",
      "Submission shape: (12065, 53)\n",
      "Validation MSE: 0.000000000000\n",
      "Min IV value: 0.154164\n",
      "Max IV value: 0.639623\n",
      "Mean IV value: 0.252602\n",
      "\n",
      "Submission Preview:\n",
      "   timestamp  call_iv_24000  call_iv_24100  call_iv_24200  call_iv_24300  \\\n",
      "0          0       0.281246       0.271402       0.261704       0.252007   \n",
      "1          1       0.272782       0.273133       0.259125       0.252000   \n",
      "2          2       0.262930       0.251744       0.238764       0.226914   \n",
      "3          3       0.242124       0.231373       0.220681       0.210894   \n",
      "4          4       0.235549       0.229293       0.223170       0.214283   \n",
      "\n",
      "   call_iv_24400  call_iv_24500  call_iv_24600  call_iv_24700  call_iv_24800  \\\n",
      "0       0.242309       0.237369       0.232507       0.227748       0.223075   \n",
      "1       0.244991       0.239305       0.233641       0.228298       0.229049   \n",
      "2       0.215065       0.204750       0.194770       0.188160       0.184421   \n",
      "3       0.198802       0.186395       0.176455       0.166558       0.161641   \n",
      "4       0.206926       0.199606       0.192717       0.187789       0.182895   \n",
      "\n",
      "   ...  put_iv_24600  put_iv_24700  put_iv_24800  put_iv_24900  put_iv_25000  \\\n",
      "0  ...      0.232269      0.227594      0.223079      0.227491      0.234353   \n",
      "1  ...      0.233460      0.228224      0.229077      0.230078      0.239820   \n",
      "2  ...      0.194503      0.187991      0.184299      0.180680      0.181409   \n",
      "3  ...      0.176129      0.166315      0.161619      0.165109      0.168599   \n",
      "4  ...      0.192522      0.187628      0.182734      0.177906      0.177015   \n",
      "\n",
      "   put_iv_25100  put_iv_25200  put_iv_25300  put_iv_25400  put_iv_25500  \n",
      "0      0.245444      0.251050      0.262128      0.273272      0.282806  \n",
      "1      0.249541      0.257963      0.266709      0.275455      0.284345  \n",
      "2      0.185223      0.190855      0.197285      0.206472      0.215659  \n",
      "3      0.172188      0.181586      0.191078      0.200570      0.206796  \n",
      "4      0.176048      0.177853      0.178941      0.182468      0.188846  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "\n",
      "Submission saved to 'submission.csv'\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Validation MSE: {validation_mse:.12f}\")\n",
    "print(f\"Min IV value: {submission[iv_columns].min().min():.6f}\")\n",
    "print(f\"Max IV value: {submission[iv_columns].max().max():.6f}\")\n",
    "print(f\"Mean IV value: {submission[iv_columns].mean().mean():.6f}\")\n",
    "\n",
    "print(\"\\nSubmission Preview:\")\n",
    "print(submission.head())\n",
    "\n",
    "print(f\"\\nSubmission saved to 'submission.csv'\")\n",
    "print(\"Process completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
